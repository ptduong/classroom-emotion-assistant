<!DOCTYPE html>
<html>
<head>
  <title>Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</title>
  <meta charset="UTF-8">
  <style>
    body { font-family: Arial, sans-serif; text-align: center; background: #f0f8ff; }
    .robot-header { font-size: 2em; margin-top: 20px; color: #2a6ebb; }
    video { width: 320px; height: 240px; border: 2px solid #2a6ebb; margin-top: 10px; }
    .emotion-box { display: flex; justify-content: center; gap: 40px; margin-top: 20px; }
    .emotion-result { border: 2px solid #2a6ebb; border-radius: 10px; padding: 20px; width: 250px; background: #fff; }
    .emotion-label { font-size: 1.5em; color: #e67e22; }
    .robot-action { margin-top: 30px; font-size: 1.2em; color: #16a085; }
    #startBtn { margin-top: 20px; padding: 10px 30px; font-size: 1.2em; background: #2a6ebb; color: #fff; border: none; border-radius: 8px; }
  </style>
</head>
<body>
  <div class="robot-header">ü§ñ Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</div>
  <video id="webcam" autoplay playsinline></video>
  <canvas id="canvas" width="320" height="240" style="display:none;"></canvas>
  <div class="emotion-box">
    <div class="emotion-result">
      <div>Bi·ªÉu c·∫£m khu√¥n m·∫∑t</div>
      <div id="imageEmotion" class="emotion-label">ƒêang nh·∫≠n di·ªán...</div>
    </div>
    <div class="emotion-result">
      <div>Gi·ªçng n√≥i</div>
      <div id="audioEmotion" class="emotion-label">ƒêang nh·∫≠n di·ªán...</div>
    </div>
  </div>
  <div class="robot-action" id="robotAction"></div>
  <button id="startBtn" onclick="startAssistant()">B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi Robot</button>

  <!-- Th∆∞ vi·ªán AI -->
  </script> https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest</script>
  </script> https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js</script>
  </script> https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js</script>

  <script>
    // Thay b·∫±ng link model c·ªßa b·∫°n
    <!-- const imageModelURL = "YOUR_IMAGE_MODEL_URL"; -->
    <!-- const audioModelURL = "YOUR_AUDIO_MODEL_URL"; -->
	const imageModelURL = "https://teachablemachine.withgoogle.com/models/vwVERB4xC/";
    const audioModelURL = "https://teachablemachine.withgoogle.com/models/TcB0ZWXJh/model.json";
    const audioMetadataURL = audioModelURL.replace("model.json", "metadata.json");

    let imageModel, audioModel;
    let webcam, canvas, ctx;

    // H√†m chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i
    function speak(text) {
      if ('speechSynthesis' in window) {
        const msg = new SpeechSynthesisUtterance(text);
        msg.lang = 'vi-VN'; // Ti·∫øng Vi·ªát
        window.speechSynthesis.cancel(); // D·ª´ng m·ªçi ph√°t hi·ªán t·∫°i
        window.speechSynthesis.speak(msg);
      }
    }

    // Logic ph·∫£n h·ªìi c·ªßa robot d·ª±a tr√™n c·∫£m x√∫c
    function robotRespond(emotion) {
      const actionDiv = document.getElementById("robotAction");
      let response = "";
      if (emotion === "Bu·ªìn" || emotion === "Sad") {
        response = "ü§ó Robot: B·∫°n ƒëang bu·ªìn √†? H√£y k·ªÉ cho m√¨nh nghe nh√©! M√¨nh s·∫Ω k·ªÉ chuy·ªán vui cho b·∫°n nghe!";
      } else if (emotion === "Lo l·∫Øng" || emotion === "Anxious") {
        response = "ü§ñ Robot: ƒê·ª´ng lo, m·ªçi chuy·ªán s·∫Ω ·ªïn th√¥i! H√≠t th·ªü s√¢u n√†o!";
      } else if (emotion === "Vui" || emotion === "Happy") {
        response = "üòÑ Robot: Tuy·ªát v·ªùi! B·∫°n ƒëang r·∫•t vui, h√£y lan t·ªèa nƒÉng l∆∞·ª£ng t√≠ch c·ª±c nh√©!";
      } else if (emotion === "T·ª©c gi·∫≠n" || emotion === "Angry") {
        response = "üòå Robot: Khi n√†o b·∫°n b√¨nh tƒ©nh l·∫°i, m√¨nh s·∫Ω c√πng b·∫°n ch∆°i tr√≤ ch∆°i th∆∞ gi√£n nh√©!";
      } else {
        response = "ü§ñ Robot: M√¨nh lu√¥n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n!";
      }
      actionDiv.innerHTML = response;
      speak(response); // Ph√°t ra loa
    }

    async function startAssistant() {
      webcam = document.getElementById("webcam");
      canvas = document.getElementById("canvas");
      ctx = canvas.getContext("2d");

      // Kh·ªüi ƒë·ªông webcam
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      webcam.srcObject = stream;

      // Load m√¥ h√¨nh h√¨nh ·∫£nh
      imageModel = await tmImage.load(imageModelURL + "model.json", imageModelURL + "metadata.json");

      // Load m√¥ h√¨nh √¢m thanh
      audioModel = await speechCommands.create("BROWSER_FFT", undefined, audioModelURL, audioMetadataURL);
      await audioModel.ensureModelLoaded();

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua √¢m thanh
      audioModel.listen(result => {
        const scores = result.scores;
        const labels = audioModel.wordLabels();
        const topIndex = scores.indexOf(Math.max(...scores));
        const audioEmotion = labels[topIndex];
        document.getElementById("audioEmotion").innerText = audioEmotion;
        robotRespond(audioEmotion);
      }, { probabilityThreshold: 0.75 });

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua h√¨nh ·∫£nh
      predictImageLoop();
    }

    async function predictImageLoop() {
      ctx.drawImage(webcam, 0, 0, canvas.width, canvas.height);
      const prediction = await imageModel.predict(canvas);
      const topPrediction = prediction.reduce((prev, current) => (prev.probability > current.probability) ? prev : current);
      document.getElementById("imageEmotion").innerText = topPrediction.className;
      robotRespond(topPrediction.className);
      setTimeout(predictImageLoop, 1000); // D·ª± ƒëo√°n m·ªói gi√¢y
    }
  </script>
</body>
</html>
