<!DOCTYPE html>

<html>
<head>
<title>Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</title>
<meta charset="utf-8"/>
<style>
    body { font-family: Arial, sans-serif; text-align: center; background: #f0f8ff; }
    .robot-header { font-size: 2em; margin-top: 20px; color: #2a6ebb; }
    video { width: 320px; height: 240px; border: 2px solid #2a6ebb; margin-top: 10px; }
    .emotion-box { display: flex; justify-content: center; gap: 40px; margin-top: 20px; }
    .emotion-result { border: 2px solid #2a6ebb; border-radius: 10px; padding: 20px; width: 250px; background: #fff; }
    .emotion-label { font-size: 1.5em; color: #e67e22; }
    .robot-action { margin-top: 30px; font-size: 1.2em; color: #16a085; }
    #startBtn { margin-top: 20px; padding: 10px 30px; font-size: 1.2em; background: #2a6ebb; color: #fff; border: none; border-radius: 8px; }
  </style>
</head>
<body>
<div class="robot-header">ü§ñ Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</div>
<video autoplay="" id="webcam" playsinline=""></video>
<canvas height="240" id="canvas" style="display:none;" width="320"></canvas>
<div class="emotion-box">
<div class="emotion-result">
<div>Bi·ªÉu c·∫£m khu√¥n m·∫∑t</div>
<div class="emotion-label" id="imageEmotion">ƒêang nh·∫≠n di·ªán...</div>
</div>
<div class="emotion-result">
<div>Gi·ªçng n√≥i</div>
<div class="emotion-label" id="audioEmotion">ƒêang nh·∫≠n di·ªán...</div>
</div>
</div>
<div class="robot-action" id="robotAction"></div>
<button id="startBtn" onclick="startAssistant()">B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi Robot</button>
<!-- Th∆∞ vi·ªán AI -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>
<script>
    // Thay b·∫±ng link model c·ªßa b·∫°n
    <!-- const imageModelURL = "YOUR_IMAGE_MODEL_URL"; -->
    <!-- const audioModelURL = "YOUR_AUDIO_MODEL_URL"; -->
	const imageModelURL = "https://teachablemachine.withgoogle.com/models/vwVERB4xC/";
    const audioModelURL = "https://teachablemachine.withgoogle.com/models/TcB0ZWXJh/model.json";
    const audioMetadataURL = audioModelURL.replace("model.json", "metadata.json");

    let imageModel, audioModel;
    let webcam, canvas, ctx;
    let lastSpokenEmotion = ""; // L∆∞u c·∫£m x√∫c ƒë√£ ph√°t √¢m thanh

    // H√†m chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i
    function speak(text) {
      if ('speechSynthesis' in window) {
        const msg = new SpeechSynthesisUtterance(text);
        msg.lang = 'vi-VN'; // Ti·∫øng Vi·ªát
        window.speechSynthesis.cancel(); // D·ª´ng m·ªçi ph√°t hi·ªán t·∫°i
        window.speechSynthesis.speak(msg);
      }
    }

    // Logic ph·∫£n h·ªìi c·ªßa robot d·ª±a tr√™n c·∫£m x√∫c
    function robotRespond(emotion) {
      const actionDiv = document.getElementById("robotAction");
      let response = "";
      if (emotion === "Bu·ªìn" || emotion === "Sad") {
        response = "ü§ó Robot: B·∫°n ƒëang bu·ªìn √†? H√£y k·ªÉ cho m√¨nh nghe nh√©! M√¨nh s·∫Ω k·ªÉ chuy·ªán vui cho b·∫°n nghe!";
      } else if (emotion === "Lo l·∫Øng" || emotion === "Anxious") {
        response = "ü§ñ Robot: ƒê·ª´ng lo, m·ªçi chuy·ªán s·∫Ω ·ªïn th√¥i! H√≠t th·ªü s√¢u n√†o!";
      } else if (emotion === "Vui" || emotion === "Happy") {
        response = "üòÑ Robot: Tuy·ªát v·ªùi! B·∫°n ƒëang r·∫•t vui, h√£y lan t·ªèa nƒÉng l∆∞·ª£ng t√≠ch c·ª±c nh√©!";
      } else if (emotion === "T·ª©c gi·∫≠n" || emotion === "Angry") {
        response = "üòå Robot: Khi n√†o b·∫°n b√¨nh tƒ©nh l·∫°i, m√¨nh s·∫Ω c√πng b·∫°n ch∆°i tr√≤ ch∆°i th∆∞ gi√£n nh√©!";
      } else {
        response = "ü§ñ Robot: M√¨nh lu√¥n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n!";
      }
      actionDiv.innerHTML = response;

      // Ch·ªâ ph√°t √¢m thanh khi c·∫£m x√∫c thay ƒë·ªïi
      if (emotion !== lastSpokenEmotion) {
        speak(response);
        lastSpokenEmotion = emotion;
      }
    }

    async function startAssistant() {
      webcam = document.getElementById("webcam");
      canvas = document.getElementById("canvas");
      ctx = canvas.getContext("2d");

      // Kh·ªüi ƒë·ªông webcam
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      webcam.srcObject = stream;

      // Load m√¥ h√¨nh h√¨nh ·∫£nh
      imageModel = await tmImage.load(imageModelURL + "model.json", imageModelURL + "metadata.json");

      // Load m√¥ h√¨nh √¢m thanh
      audioModel = await speechCommands.create("BROWSER_FFT", undefined, audioModelURL, audioMetadataURL);
      await audioModel.ensureModelLoaded();

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua √¢m thanh
      audioModel.listen(result => {
        const scores = result.scores;
        const labels = audioModel.wordLabels();
        const topIndex = scores.indexOf(Math.max(...scores));
        const audioEmotion = labels[topIndex];
        document.getElementById("audioEmotion").innerText = audioEmotion;
        robotRespond(audioEmotion);
      }, { probabilityThreshold: 0.75 });

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua h√¨nh ·∫£nh
      predictImageLoop();
    }

    async function predictImageLoop() {
      ctx.drawImage(webcam, 0, 0, canvas.width, canvas.height);
      const prediction = await imageModel.predict(canvas);
      const topPrediction = prediction.reduce((prev, current) => (prev.probability > current.probability) ? prev : current);
      document.getElementById("imageEmotion").innerText = topPrediction.className;
      robotRespond(topPrediction.className);
      setTimeout(predictImageLoop, 1000); // D·ª± ƒëo√°n m·ªói gi√¢y
    }
  </script>

<script>
// H√†m debounce
function debounce(func, wait) {
  let timeout;
  return function(...args) {
    clearTimeout(timeout);
    timeout = setTimeout(() => func.apply(this, args), wait);
  };
}

// H√†m speak t·ªëi ∆∞u ti·∫øng Vi·ªát
function speakVietnamese(text) {
  if ('speechSynthesis' in window) {
    const utter = new SpeechSynthesisUtterance(text);
    const voices = window.speechSynthesis.getVoices();
    utter.lang = 'vi-VN';
    utter.voice = voices.find(v => v.lang === 'vi-VN') || null;
    window.speechSynthesis.speak(utter);
  }
}

// Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u c·∫£m x√∫c ƒë√£ ph√°t √¢m
let lastSpokenEmotion = null;

// H√†m ph·∫£n h·ªìi c·ªßa robot, t√≠ch h·ª£p debounce v√† speakVietnamese
const robotRespond = debounce(function(emotion) {
  const actionDiv = document.getElementById("robotAction");
  let response = "";
  if (emotion === "Bu·ªìn" || emotion === "Sad") {
    response = "ü§ó Robot: B·∫°n ƒëang bu·ªìn √†? H√£y k·ªÉ cho m√¨nh nghe nh√©! M√¨nh s·∫Ω k·ªÉ chuy·ªán vui cho b·∫°n nghe!";
  } else if (emotion === "Lo l·∫Øng" || emotion === "Anxious") {
    response = "ü§ñ Robot: ƒê·ª´ng lo, m·ªçi chuy·ªán s·∫Ω ·ªïn th√¥i! H√≠t th·ªü s√¢u n√†o!";
  } else if (emotion === "Vui" || emotion === "Happy") {
    response = "üòÑ Robot: Tuy·ªát v·ªùi! B·∫°n ƒëang r·∫•t vui, h√£y lan t·ªèa nƒÉng l∆∞·ª£ng t√≠ch c·ª±c nh√©!";
  } else if (emotion === "T·ª©c gi·∫≠n" || emotion === "Angry") {
    response = "üòå Robot: Khi n√†o b·∫°n b√¨nh tƒ©nh l·∫°i, m√¨nh s·∫Ω c√πng b·∫°n ch∆°i tr√≤ ch∆°i th∆∞ gi√£n nh√©!";
  } else {
    response = "ü§ñ Robot: M√¨nh lu√¥n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n!";
  }
  actionDiv.innerHTML = response;

  // Ch·ªâ ph√°t √¢m thanh khi c·∫£m x√∫c thay ƒë·ªïi
  if (emotion !== lastSpokenEmotion) {
    speakVietnamese(response);
    lastSpokenEmotion = emotion;
  }
}, 500);
</script>
</body>
</html>
