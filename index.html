<!DOCTYPE html>
<html>
<head>
  <title>Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</title>
  <meta charset="UTF-8">
  <style>
    body { font-family: Arial, sans-serif; text-align: center; background: #f0f8ff; }
    .robot-header { font-size: 2em; margin-top: 20px; color: #2a6ebb; }
    video { width: 320px; height: 240px; border: 2px solid #2a6ebb; margin-top: 10px; }
    .emotion-box { display: flex; justify-content: center; gap: 40px; margin-top: 20px; }
    .emotion-result { border: 2px solid #2a6ebb; border-radius: 10px; padding: 20px; width: 250px; background: #fff; }
    .emotion-label { font-size: 1.5em; color: #e67e22; }
    .robot-action { margin-top: 30px; font-size: 1.2em; color: #16a085; }
    #startBtn { margin-top: 20px; padding: 10px 30px; font-size: 1.2em; background: #2a6ebb; color: #fff; border: none; border-radius: 8px; }
  </style>
</head>
<body>
  <div class="robot-header">ü§ñ Robot Tr·ª£ L√Ω T√¢m L√Ω H·ªçc sinh</div>
  <video id="webcam" autoplay playsinline></video>
  <canvas id="canvas" width="320" height="240" style="display:none;"></canvas>
  <div class="emotion-box">
    <div class="emotion-result">
      <div>Bi·ªÉu c·∫£m khu√¥n m·∫∑t</div>
      <div id="imageEmotion" class="emotion-label">ƒêang nh·∫≠n di·ªán...</div>
    </div>
    <div class="emotion-result">
      <div>Gi·ªçng n√≥i</div>
      <div id="audioEmotion" class="emotion-label">ƒêang nh·∫≠n di·ªán...</div>
    </div>
  </div>
  <div class="robot-action" id="robotAction"></div>
  <button id="startBtn" onclick="startAssistant()">B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi Robot</button>

  <!-- Th∆∞ vi·ªán AI -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>

  <script>
    // Thay b·∫±ng link model c·ªßa b·∫°n
    <!-- const imageModelURL = "YOUR_IMAGE_MODEL_URL"; -->
    <!-- const audioModelURL = "YOUR_AUDIO_MODEL_URL"; -->
	const imageModelURL = "https://teachablemachine.withgoogle.com/models/vwVERB4xC/";
    const audioModelURL = "https://teachablemachine.withgoogle.com/models/TcB0ZWXJh/model.json";
    const audioMetadataURL = audioModelURL.replace("model.json", "metadata.json");

    let imageModel, audioModel;
    let webcam, canvas, ctx;
    let lastSpokenEmotion = ""; // L∆∞u c·∫£m x√∫c ƒë√£ ph√°t √¢m thanh

    // H√†m chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i
    function speak(text) {
      if ('speechSynthesis' in window) {
        const msg = new SpeechSynthesisUtterance(text);
        msg.lang = 'vi-VN'; // Ti·∫øng Vi·ªát
        window.speechSynthesis.cancel(); // D·ª´ng m·ªçi ph√°t hi·ªán t·∫°i
        window.speechSynthesis.speak(msg);
      }
    }

    // Logic ph·∫£n h·ªìi c·ªßa robot d·ª±a tr√™n c·∫£m x√∫c
    function robotRespond(emotion) {
      const actionDiv = document.getElementById("robotAction");
      let response = "";
      if (emotion === "Bu·ªìn" || emotion === "Sad") {
        response = "ü§ó Robot: B·∫°n ƒëang bu·ªìn √†? H√£y k·ªÉ cho m√¨nh nghe nh√©! M√¨nh s·∫Ω k·ªÉ chuy·ªán vui cho b·∫°n nghe!";
      } else if (emotion === "Lo l·∫Øng" || emotion === "Anxious") {
        response = "ü§ñ Robot: ƒê·ª´ng lo, m·ªçi chuy·ªán s·∫Ω ·ªïn th√¥i! H√≠t th·ªü s√¢u n√†o!";
      } else if (emotion === "Vui" || emotion === "Happy") {
        response = "üòÑ Robot: Tuy·ªát v·ªùi! B·∫°n ƒëang r·∫•t vui, h√£y lan t·ªèa nƒÉng l∆∞·ª£ng t√≠ch c·ª±c nh√©!";
      } else if (emotion === "T·ª©c gi·∫≠n" || emotion === "Angry") {
        response = "üòå Robot: Khi n√†o b·∫°n b√¨nh tƒ©nh l·∫°i, m√¨nh s·∫Ω c√πng b·∫°n ch∆°i tr√≤ ch∆°i th∆∞ gi√£n nh√©!";
      } else {
        response = "ü§ñ Robot: M√¨nh lu√¥n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n!";
      }
      actionDiv.innerHTML = response;

      // Ch·ªâ ph√°t √¢m thanh khi c·∫£m x√∫c thay ƒë·ªïi
      if (emotion !== lastSpokenEmotion) {
        speak(response);
        lastSpokenEmotion = emotion;
      }
    }

    async function startAssistant() {
      webcam = document.getElementById("webcam");
      canvas = document.getElementById("canvas");
      ctx = canvas.getContext("2d");

      // Kh·ªüi ƒë·ªông webcam
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      webcam.srcObject = stream;

      // Load m√¥ h√¨nh h√¨nh ·∫£nh
      imageModel = await tmImage.load(imageModelURL + "model.json", imageModelURL + "metadata.json");

      // Load m√¥ h√¨nh √¢m thanh
      audioModel = await speechCommands.create("BROWSER_FFT", undefined, audioModelURL, audioMetadataURL);
      await audioModel.ensureModelLoaded();

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua √¢m thanh
      audioModel.listen(result => {
        const scores = result.scores;
        const labels = audioModel.wordLabels();
        const topIndex = scores.indexOf(Math.max(...scores));
        const audioEmotion = labels[topIndex];
        document.getElementById("audioEmotion").innerText = audioEmotion;
        robotRespond(audioEmotion);
      }, { probabilityThreshold: 0.75 });

      // Nh·∫≠n di·ªán c·∫£m x√∫c qua h√¨nh ·∫£nh
      predictImageLoop();
    }

    async function predictImageLoop() {
      ctx.drawImage(webcam, 0, 0, canvas.width, canvas.height);
      const prediction = await imageModel.predict(canvas);
      const topPrediction = prediction.reduce((prev, current) => (prev.probability > current.probability) ? prev : current);
      document.getElementById("imageEmotion").innerText = topPrediction.className;
      robotRespond(topPrediction.className);
      setTimeout(predictImageLoop, 1000); // D·ª± ƒëo√°n m·ªói gi√¢y
    }
  </script>
</body>
</html>
<div style="margin-top: 20px;">
  <h2>üé§ Tr·ª£ l√Ω ph√°t √¢m thanh</h2>
  <button onclick="speakMessage(0)">Ch√†o h·ªèi</button>
  <button onclick="speakMessage(1)">Nh·∫≠n di·ªán c·∫£m x√∫c</button>
  <button onclick="speakMessage(2)">Ph·∫£n h·ªìi c·∫£m x√∫c vui</button>
  <button onclick="speakMessage(3)">Khuy·∫øn kh√≠ch chia s·∫ª</button>
  <button onclick="speakMessage(4)">C·∫£m ∆°n chia s·∫ª</button>
  <button onclick="speakMessage(5)">K·∫øt th√∫c tr√≤ chuy·ªán</button>
</div>

<script>
  const messages = [
    "Xin ch√†o! M√¨nh l√† Tr·ª£ l√Ω T√¢m l√Ω c·ªßa b·∫°n. B·∫°n mu·ªën chia s·∫ª ƒëi·ªÅu g√¨ h√¥m nay kh√¥ng?",
    "M√¨nh ƒëang nh·∫≠n di·ªán c·∫£m x√∫c c·ªßa b·∫°n, vui l√≤ng ch·ªù m·ªôt ch√∫t nh√©.",
    "M√¨nh th·∫•y b·∫°n c√≥ v·∫ª ƒëang vui. B·∫°n mu·ªën n√≥i v·ªÅ ƒëi·ªÅu ƒë√≥ kh√¥ng?",
    "N·∫øu b·∫°n c·∫£m th·∫•y kh√¥ng ·ªïn, h√£y n√≥i v·ªõi m√¨nh nh√©. M√¨nh lu√¥n ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe.",
    "C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª. M√¨nh ƒë√£ ghi nh·∫≠n c·∫£m x√∫c c·ªßa b·∫°n.",
    "C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán v·ªõi m√¨nh. Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh!"
  ];

  function speakMessage(index) {
    const utter = new SpeechSynthesisUtterance(messages[index]);
    utter.lang = 'vi-VN';

    const voices = window.speechSynthesis.getVoices();
    const viVoices = voices.filter(v => v.lang === 'vi-VN');
    if (viVoices.length > 0) {
      const femaleVoice = viVoices.find(v => v.name.toLowerCase().includes('female') || v.name.toLowerCase().includes('n·ªØ'));
      utter.voice = femaleVoice || viVoices[0];
    }

    utter.rate = 1;
    utter.pitch = 1.1;
    utter.volume = 1;

    window.speechSynthesis.speak(utter);
  }

  window.speechSynthesis.onvoiceschanged = () => {};
</script>
